from __future__ import print_function
from __future__ import division
from __future__ import absolute_import
import tensorflow as tf
import numpy as np
import config
import sys

FLAGS = config.flags.FLAGS


# The function first generates an encoder network for each predator agent's observation 
# using the encoder_network() function. It then generates a decoder network that takes in the encoded observations
# of all predator agents and the communication schedule for the current step and generates a concatenated 
# communication message for all predator agents using the decode_concat_network() function.

def generate_comm_network(obs_list, obs_dim_per_unit, action_dim, n_agent, trainable=True, share=False, schedule=None):
    actions = list()
    h_num = 32

    capacity = FLAGS.capa 

    # Generate encoder
    encoder_scope = "encoder"
    aggr_scope = "aggr"
    decoder_out_dim = 16
    encoder_list = list()

    for i in range(n_agent):
        if not FLAGS.e_share:
            encoder_scope = "encoder" + str(i)
        # Statement creates a variable scope for the encoder network. A variable scope is a way to organize the variables used in a
        # TensorFlow graph, and it allows you to reuse variables across different parts of the graph.
        with tf.compat.v1.variable_scope(encoder_scope):
            encoder = encoder_network(obs_list[i], capacity, 32, 1)
        encoder_list.append(encoder)

    aggr_list = list()
    if not FLAGS.a_share:
        for i in range(n_agent):
            aggr_scope = "aggr" + str(i)
            with tf.compat.v1.variable_scope(aggr_scope):
                aggr_out = decode_concat_network(encoder_list, schedule, capacity, decoder_out_dim)
            aggr_list.append(aggr_out)

    else:
        with tf.variable_scope(aggr_scope):
            aggr_out = decode_concat_network(encoder_list, schedule, capacity, decoder_out_dim)
        for i in range(n_agent):
            aggr_list.append(aggr_out)

    # Generate actor
    scope = "comm"
    for i in range(n_agent):
        if not FLAGS.s_share:
            scope = "comm" + str(i)

        with tf.compat.v1.variable_scope(scope):
            agent_actor = comm_encoded_obs(obs_list[i], aggr_list[i], action_dim, h_num, trainable)

        actions.append(agent_actor)

    return tf.concat(actions, axis=-1)

#  the function generates an actor network for each predator agent's communication action
#  using the comm_encoded_obs() function, which takes in the observation of the predator
#  agent, the concatenated communication message generated by the decoder network, and generates a
#  probability distribution over communication actions for the predator agent.
#  encoder_network() is used to encode the observation of each predator agent into 
#  a lower-dimensional representation that can be used as input to the decoder network.
#  It takes in the observation tensor e_input and generates an encoded representation of size out_
#  dim using fully connected layers with h_num hidden units and h_level layers. The output 
#  of encoder_network() is a tensor of shape (batch_size, out_dim).

# Action selector: 
def comm_encoded_obs(obs, c_input, action_dim, h_num, trainable=True):
    c_input = tf.concat([obs, c_input], axis=1)
    hidden_1 = tf.keras.layers.Dense(units=h_num, activation=tf.nn.relu,
                                     kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
                                     bias_initializer=tf.constant_initializer(0.1),  # biases
                                     use_bias=True, trainable=trainable, name='sender_1')(c_input)
    hidden_2 = tf.keras.layers.Dense(units=h_num, activation=tf.nn.relu,
                                     kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
                                     bias_initializer=tf.constant_initializer(0.1),  # biases
                                     use_bias=True, trainable=trainable, name='sender_2')(hidden_1)

    hidden_3 = tf.keras.layers.Dense(units=h_num, activation=tf.nn.relu,
                                     kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
                                     bias_initializer=tf.constant_initializer(0.1),  # biases
                                     use_bias=True, trainable=trainable, name='sender_3')(hidden_2)

    a = tf.keras.layers.Dense(units=action_dim, activation=tf.nn.softmax,
                              kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
                              bias_initializer=tf.constant_initializer(0.1),  # biases
                              use_bias=True, trainable=trainable, name='sender_4')(hidden_3)
    return a

# Encoding
# used to generate a probability distribution over communication actions for 
# each predator agent, given its observation and the concatenated communication message generated by the decoder network
# out_dim is capacity of the communication, h_num is number of neurons in each layer andh_level is the number of layers.
# For example, the output may be 10 which tells us that, the capacity of the communication is 2. 
# This is like feature extraction that maps observation into lower dimensions representations of it. 

def encoder_network(e_input, out_dim, h_num, h_level, name="encoder", trainable=True):
    hidden = e_input
    for i in range(h_level):
        # Multi-Head Self-Attention Layer
        hidden_expanded = tf.expand_dims(hidden, axis=1)  # Add sequence_length dimension
        hidden = tf.keras.layers.MultiHeadAttention(
            num_heads=4, key_dim=h_num, trainable=trainable, name=name+"_self_attention_"+str(i)
        )(hidden_expanded, hidden_expanded)
        hidden = tf.squeeze(hidden, axis=1)  # Remove sequence_length dimension
        hidden = tf.keras.layers.LayerNormalization(epsilon=1e-6, trainable=trainable)(hidden)
        
        # Position-Wise Feed-Forward Layer
        ff_hidden = tf.keras.layers.Dense(units=h_num, activation=tf.nn.relu,
                                          kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
                                          bias_initializer=tf.constant_initializer(0.1),  # biases
                                          use_bias=True, trainable=trainable, name=name+"_ffnn_"+str(i))(hidden)
        ff_hidden = tf.keras.layers.LayerNormalization(epsilon=1e-6, trainable=trainable)(ff_hidden)

        # Linear projection layer to adjust dimensions of hidden
        hidden_projection = tf.keras.layers.Dense(units=h_num, trainable=trainable, name=name+"_projection_"+str(i))(hidden)
        
        # Add skip connection
        hidden = hidden_projection + ff_hidden

    ret = tf.keras.layers.Dense(units=out_dim,
                                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
                                bias_initializer=tf.constant_initializer(0.1),  # biases
                                use_bias=True, trainable=trainable, name=name+"_out")(hidden)
    return ret
# Suppose we have a multi-agent predator-prey game with 3 predator agents and a communication capacity of 2. At each step of the game, each predator agent generates an observation of the game environment, which is encoded into a tensor of shape (batch_size, capacity) using the encoder_network() function.

# The encoded observation tensors for the 3 predator agents are stored in a list m_input_list, which has length 3. The communication schedule tensor schedule is a binary tensor of shape (batch_size, 3), where each element indicates whether the corresponding predator agent should communicate at the current step. For example, if schedule is [1, 0, 1], this means that the first and third predator agents should communicate at the current step, while the second predator agent should not communicate.

# The decode_concat_network() function takes m_input_list, schedule, capacity, and out_dim as input. The first step of the function is to concatenate the encoded observation tensors in m_input_list along the second-to-last dimension using tf.stack(). This results in a tensor inp with shape (batch_size, 3, 2), where the last dimension corresponds to the communication capacity.

# Next, the function applies a boolean mask to the inp tensor to select only the communication messages that are scheduled for the current step, based on the values in the schedule tensor. The boolean mask is generated by reshaping the schedule tensor into a 1D boolean tensor with the same length as the flattened inp tensor, and then using tf.boolean_mask() to select the appropriate elements of the flattened inp.

# For example, suppose the schedule tensor is [0, 1, 1, 1, 0, 1] and the flattened inp tensor is [1, 2, 3, 4, 5, 6]. The boolean mask is [2, 3, 4, 6], which corresponds to the communication messages from the second, third, and sixth elements of the flattened inp tensor.

# Finally, the function reshapes the resulting masked communication messages tensor into a 2D tensor of shape (batch_size, 3 * 2), where each row represents the concatenated communication message for all predator agents at the current step, and each column represents the communication message for a single predator agent.

# For example, if the masked communication messages tensor is [2, 3, 4, 6, 8, 9], the reshaped tensor will be [[2, 3, 4, 6, 8, 9]], because there is only one example in the batch. The first two elements of the tensor correspond to the communication message from the second predator agent, the next two elements correspond to the communication message from the third predator agent, and the final two elements correspond to the communication message from the sixth predator agent.
# The decode_concat_network() function returns the reshaped tensor, which can then be used as input to the comm_encoded_obs() function to generate a probability distribution over the possible communication actions, as I explained in my previous response.


def decode_concat_network(m_input_list, schedule, capacity, out_dim):

    inp = tf.stack(m_input_list, axis=-2)
    print(inp)
    masked_msg = tf.boolean_mask(tf.reshape(inp, [-1, capacity]), tf.reshape(tf.cast(schedule, tf.bool), [-1]))
    return tf.reshape(masked_msg, [-1, FLAGS.s_num * capacity], name='scheduled')
